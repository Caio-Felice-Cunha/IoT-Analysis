{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e32795-efee-4635-ab3b-96a69ac3e973",
   "metadata": {},
   "source": [
    "# <font color='blue'>IoT-Analysis</font>\n",
    "\n",
    "### <font>Real-Time IoT Sensor Data Analysis with Apache Spark Streaming and Apache Kafka</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3183aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark and initialize\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14c6950-525d-4e54-a25e-13e9d6818c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29efd4a",
   "metadata": {},
   "source": [
    "> We need to add Spark Streaming Integration Connector with Apache Kafka. Pay attention to the version of PySpark being used.\n",
    "\n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5172de34-9787-4f22-b1a7-8e9bb3b7cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conector\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efeedac",
   "metadata": {},
   "source": [
    "## Creating the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35f1973-363c-4112-8440-62fca2410984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Spark session\n",
    "spark = SparkSession.builder.appName(\"IoT-Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8744ef",
   "metadata": {},
   "source": [
    "## Read Kafka Spark Structured Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa0de89c-e700-4754-9175-d95777b95a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a subscription to the topic that has the data stream we want to \"pull\" the data from.\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"IotAnalysis\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60991a",
   "metadata": {},
   "source": [
    "## Data Source Schema Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92c2301-7f4c-48fd-8e5a-cc12384ad834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the schema of the data we want to capture for analysis (temperature)\n",
    "data_schema_temp = StructType([StructField(\"reading\", \n",
    "                                             StructType([StructField(\"temperature\", DoubleType(), True)]), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f0a8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the global data schema in the stream\n",
    "data_schema = StructType([ \n",
    "    StructField(\"id_sensor\", StringType(), True), \n",
    "    StructField(\"equipment_id\", StringType(), True), \n",
    "    StructField(\"sensor\", StringType(), True), \n",
    "    StructField(\"date_event\", StringType(), True), \n",
    "    StructField(\"standard\", data_schema_temp, True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf910ee0",
   "metadata": {},
   "source": [
    "## Parse the Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a505e1-6004-48ab-8275-0cdb96de583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture each line of data (each value) as a string\n",
    "df_conversion = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bcc58eb-a9b5-4760-8c09-abe6bdd117b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON format into dataframe\n",
    "df_conversion = df_conversion.withColumn(\"jsonData\", from_json(col(\"value\"), data_schema)).select(\"jsonData.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77250a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_sensor: string (nullable = true)\n",
      " |-- equipment_id: string (nullable = true)\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- date_event: string (nullable = true)\n",
      " |-- standard: struct (nullable = true)\n",
      " |    |-- reading: struct (nullable = true)\n",
      " |    |    |-- temperature: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_conversion.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d91c83",
   "metadata": {},
   "source": [
    "## Prepare the Dataframe\n",
    "\n",
    "This dataframe is in the format we need for parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9734cc6-fd90-4f32-aa7f-dafcf5bc5688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We renamed the columns to simplify our analysis\n",
    "df_conversion_temp_sensor = df_conversion.select(col(\"standard.reading.temperature\").alias(\"temperature\"), \n",
    "                                               col(\"sensor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d725e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- sensor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_conversion_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4ea01",
   "metadata": {},
   "source": [
    "## Real Time Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16da9087-67a4-4f16-8325-7d25dcd8d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have the object that will contain our analysis, the calculation of average temperatures per sensor\n",
    "df_avg_temp_sensor = df_conversion_temp_sensor.groupby(\"sensor\").mean(\"temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c515750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- avg(temperature): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_avg_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c666eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We renamed the columns to simplify our analysis\n",
    "df_avg_temp_sensor = df_avg_temp_sensor.select(col(\"sensor\").alias(\"sensor\"), \n",
    "                                                   col(\"avg(temperature)\").alias(\"avg_temp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59dedc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- avg_temp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_avg_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe8621",
   "metadata": {},
   "source": [
    "Below we open the streaming for real-time data analysis, printing the result on the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3946d80-165e-47a5-a921-56e3d6e675d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object that starts querying the streaming in console format\n",
    "query = df_avg_temp_sensor.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e8a6d2",
   "metadata": {},
   "source": [
    "Upload new files to Kafka to see the real-time analysis over here. Click the Stop button in the top menu to stop the cell at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5792c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the streaming query and prevent the process from being terminated\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41028bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ae4ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b4e69b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@464e5a35, org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy$$Lambda$2336/0x00000001010f5840@714443c7\n",
      "+- *(4) HashAggregate(keys=[sensor#28], functions=[avg(temperature#36)])\n",
      "   +- StateStoreSave [sensor#28], state info [ checkpoint = file:/C:/Users/Caio/AppData/Local/Temp/temporary-b8fdda09-504d-4ab0-9bcc-e8aeffb2747b/state, runId = 7096587f-a337-40de-94e9-34a963c99a5c, opId = 0, ver = 0, numPartitions = 200], Complete, 0, 2\n",
      "      +- *(3) HashAggregate(keys=[sensor#28], functions=[merge_avg(temperature#36)])\n",
      "         +- StateStoreRestore [sensor#28], state info [ checkpoint = file:/C:/Users/Caio/AppData/Local/Temp/temporary-b8fdda09-504d-4ab0-9bcc-e8aeffb2747b/state, runId = 7096587f-a337-40de-94e9-34a963c99a5c, opId = 0, ver = 0, numPartitions = 200], 2\n",
      "            +- *(2) HashAggregate(keys=[sensor#28], functions=[merge_avg(temperature#36)])\n",
      "               +- Exchange hashpartitioning(sensor#28, 200), ENSURE_REQUIREMENTS, [plan_id=51]\n",
      "                  +- *(1) HashAggregate(keys=[sensor#28], functions=[partial_avg(temperature#36)])\n",
      "                     +- Project [from_json(StructField(id_sensor,StringType,true), StructField(equipment_id,StringType,true), StructField(sensor,StringType,true), StructField(date_event,StringType,true), StructField(standard,StructType(StructField(reading,StructType(StructField(temperature,DoubleType,true)),true)),true), cast(value#8 as string), Some(America/Sao_Paulo)).standard.reading.temperature AS temperature#36, from_json(StructField(id_sensor,StringType,true), StructField(equipment_id,StringType,true), StructField(sensor,StringType,true), StructField(date_event,StringType,true), StructField(standard,StructType(StructField(reading,StructType(StructField(temperature,DoubleType,true)),true)),true), cast(value#8 as string), Some(America/Sao_Paulo)).sensor AS sensor#28]\n",
      "                        +- MicroBatchScan[key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13] class org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e469f93",
   "metadata": {},
   "source": [
    "## Real Time Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65867a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object that starts the query to the stream with memory format (creates a temporary table)\n",
    "query_memoria = df_avg_temp_sensor \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"Iot_Analysis\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "706168fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x219fb9d4b20>,\n",
       " <pyspark.sql.streaming.StreamingQuery at 0x219fbb53190>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# streams enabled\n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep the query running for a while and apply SQL to the data in real time\n",
    "from time import sleep\n",
    "\n",
    "for x in range(10):\n",
    "    \n",
    "    spark.sql(\"select sensor, round(avg_temp, 2) as avg from Iot_Analysis where avg_temp > 65\").show()\n",
    "    sleep(3)\n",
    "    \n",
    "query_memoria.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63e53ba",
   "metadata": {},
   "source": [
    "## Disclaimer:\n",
    "A good part of this project was largely done in the Data Science Academy, Big Data Real-Time Analytics with Python and Spark course (part of the Data Scientist training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36441eef",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
